{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from config import API_KEY\n",
    "import json\n",
    "from pyspark.sql import SparkSession as ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5]\n",
    "spark = ss.builder.master(\"local[1]\") \\\n",
    "                    .appName('HelloSpark') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Jim:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HelloSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b6feef7b80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', '', ('', '', ''), ('', '', ''))\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.java_ver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1,2,3,4,5], 2)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2023|              SE|             FT|Principal Data Sc...| 80000|            EUR|        85847|                ES|         100|              ES|           L|\n",
      "|     2023|              MI|             CT|         ML Engineer| 30000|            USD|        30000|                US|         100|              US|           S|\n",
      "|     2023|              MI|             CT|         ML Engineer| 25500|            USD|        25500|                US|         100|              US|           S|\n",
      "|     2023|              SE|             FT|      Data Scientist|175000|            USD|       175000|                CA|         100|              CA|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|120000|            USD|       120000|                CA|         100|              CA|           M|\n",
      "|     2023|              SE|             FT|   Applied Scientist|222200|            USD|       222200|                US|           0|              US|           L|\n",
      "|     2023|              SE|             FT|   Applied Scientist|136000|            USD|       136000|                US|           0|              US|           L|\n",
      "|     2023|              SE|             FT|      Data Scientist|219000|            USD|       219000|                CA|           0|              CA|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|141000|            USD|       141000|                CA|           0|              CA|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|147100|            USD|       147100|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist| 90700|            USD|        90700|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|        Data Analyst|130000|            USD|       130000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|        Data Analyst|100000|            USD|       100000|                US|         100|              US|           M|\n",
      "|     2023|              EN|             FT|   Applied Scientist|213660|            USD|       213660|                US|           0|              US|           L|\n",
      "|     2023|              EN|             FT|   Applied Scientist|130760|            USD|       130760|                US|           0|              US|           L|\n",
      "|     2023|              SE|             FT|        Data Modeler|147100|            USD|       147100|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|        Data Modeler| 90700|            USD|        90700|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|170000|            USD|       170000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|      Data Scientist|150000|            USD|       150000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|        Data Analyst|150000|            USD|       150000|                US|         100|              US|           M|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"../../data/datasets/ds_salaries.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------\n",
      " work_year          | 2023                 \n",
      " experience_level   | SE                   \n",
      " employment_type    | FT                   \n",
      " job_title          | Principal Data Sc... \n",
      " salary             | 80000                \n",
      " salary_currency    | EUR                  \n",
      " salary_in_usd      | 85847                \n",
      " employee_residence | ES                   \n",
      " remote_ratio       | 100                  \n",
      " company_location   | ES                   \n",
      " company_size       | L                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See Columns and Types\n",
    "df.printSchema()\n",
    "df.columns\n",
    "\n",
    "# See Data Vertically\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# DF Length\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select DF column, Get distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           job_title|\n",
      "+--------------------+\n",
      "|3D Computer Visio...|\n",
      "|  Lead Data Engineer|\n",
      "|        Data Modeler|\n",
      "| Data Scientist Lead|\n",
      "|Principal Data Ar...|\n",
      "|Head of Machine L...|\n",
      "|Machine Learning ...|\n",
      "|Data Analytics Sp...|\n",
      "|     Data Specialist|\n",
      "|Data Operations E...|\n",
      "|Deep Learning Res...|\n",
      "| Data Analytics Lead|\n",
      "|  Power BI Developer|\n",
      "|Machine Learning ...|\n",
      "|   Lead Data Analyst|\n",
      "|        BI Developer|\n",
      "|Staff Data Scientist|\n",
      "|       ETL Developer|\n",
      "|           Data Lead|\n",
      "|Product Data Scie...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job_title').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|              3755|\n",
      "|   mean|190695.57177097205|\n",
      "| stddev| 671676.5005079063|\n",
      "|    min|              6000|\n",
      "|    max|          30400000|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|              3755|\n",
      "|   mean|190695.57177097205|\n",
      "| stddev| 671676.5005079063|\n",
      "|    min|             10000|\n",
      "|    max|             99750|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|              3755|\n",
      "|   mean|190695.57177097205|\n",
      "| stddev| 671676.5005079063|\n",
      "|    min|              6000|\n",
      "|    max|          30400000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Simply select the column\n",
    "df.select(col('salary').cast(\"int\")).describe().show()\n",
    "\n",
    "# Mutate the dataframe\n",
    "df.select('salary').describe().show()\n",
    "df = df.withColumn(\"salary\", col('salary').cast(\"int\"))\n",
    "df.select('salary').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change column type, introduce 'year' function / type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- work_year: integer (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import month, year\n",
    "\n",
    "# Change multiple columns\n",
    "# df.printSchema()\n",
    "# df.show()\n",
    "int_cols = ['salary', 'salary_in_usd', 'remote_ratio']\n",
    "df_tmp = df\n",
    "\n",
    "# df_tmp = df_tmp.select(*(col(c).cast(\"int\") for c in int_cols))\n",
    "for c in int_cols: \n",
    "    df_tmp = df_tmp.withColumn(c, col(c).cast('int'))\n",
    "\n",
    "\n",
    "df_tmp = df_tmp.withColumns({'salary'       : col('salary').cast(\"int\"),\n",
    "                             'salary_in_usd': col('salary_in_usd').cast(\"int\"),\n",
    "                             'remote_ratio' : col('remote_ratio').cast(\"int\"),\n",
    "                             'work_year'    : year(col('work_year'))})\n",
    "\n",
    "\n",
    "df_tmp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|           job_title|            salary|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|                3755|              3755|\n",
      "|   mean|                null|190695.57177097205|\n",
      "| stddev|                null| 671676.5005079063|\n",
      "|    min|3D Computer Visio...|             10000|\n",
      "|    max|Staff Data Scientist|             99750|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('job_title', 'salary').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------------+-------+\n",
      "|experience_level|           job_title|company_location|Big_Sal|\n",
      "+----------------+--------------------+----------------+-------+\n",
      "|              SE|Computer Vision E...|              US|      1|\n",
      "|              MI|Machine Learning ...|              IN|      1|\n",
      "|              SE|   Applied Scientist|              US|      1|\n",
      "|              MI|      Data Scientist|              HK|      1|\n",
      "|              SE|Machine Learning ...|              US|      1|\n",
      "|              SE|Machine Learning ...|              US|      1|\n",
      "|              MI|Applied Data Scie...|              IN|      1|\n",
      "|              SE|   Applied Scientist|              US|      1|\n",
      "|              EN|       Data Engineer|              IN|      1|\n",
      "|              EX|        Head of Data|              US|      1|\n",
      "|              SE|Machine Learning ...|              US|      1|\n",
      "|              EX|Director of Data ...|              US|      1|\n",
      "|              SE|      Data Scientist|              US|      1|\n",
      "|              SE|        AI Scientist|              IL|      1|\n",
      "|              SE|      Data Architect|              US|      1|\n",
      "|              MI|      Data Scientist|              IN|      1|\n",
      "|              MI|   Lead Data Analyst|              IN|      1|\n",
      "|              MI|  Research Scientist|              US|      1|\n",
      "|              EX|       Data Engineer|              US|      1|\n",
      "|              SE|        Data Analyst|              IN|      1|\n",
      "+----------------+--------------------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# df.show()\n",
    "# Add Boolean Column\n",
    "df2 = df.select('experience_level', \n",
    "          'job_title',\n",
    "          'company_location',\n",
    "          F.when(df.salary > 300000, 1).otherwise(0).alias('Big_Sal'))\\\n",
    "\n",
    "# Select column by condition\n",
    "df2[df2['Big_Sal'] == 1].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|     job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2023|              SE|             FT|Data Scientist|175000|            USD|       175000|                CA|         100|              CA|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|120000|            USD|       120000|                CA|         100|              CA|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|219000|            USD|       219000|                CA|           0|              CA|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|141000|            USD|       141000|                CA|           0|              CA|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|147100|            USD|       147100|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist| 90700|            USD|        90700|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|  Data Analyst|130000|            USD|       130000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|  Data Analyst|100000|            USD|       100000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|170000|            USD|       170000|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|150000|            USD|       150000|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|  Data Analyst|150000|            USD|       150000|                US|         100|              US|           M|\n",
      "|     2023|              MI|             FT|  Data Analyst|110000|            USD|       110000|                US|         100|              US|           M|\n",
      "|     2023|              MI|             FT|  Data Analyst|105380|            USD|       105380|                US|           0|              US|           M|\n",
      "|     2023|              MI|             FT|  Data Analyst| 64500|            USD|        64500|                US|           0|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|212750|            USD|       212750|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|185000|            USD|       185000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|262000|            USD|       262000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|245000|            USD|       245000|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|275300|            USD|       275300|                US|         100|              US|           M|\n",
      "|     2023|              SE|             FT|Data Scientist|183500|            USD|       183500|                US|         100|              US|           M|\n",
      "+---------+----------------+---------------+--------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select(\"job_title\", df.job_title.startswith(\"Data\").alias(\"Data\")).show()\n",
    "# df[df.job_title.startswith(\"Data\") == True].show()\n",
    "df[df.job_title.endswith(\"Scientist\") == True]\n",
    "\n",
    "\n",
    "df[df.job_title.isin([\"Data Scientist\", \"Data Analyst\"]) == True].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           job_title|count|\n",
      "+--------------------+-----+\n",
      "|       Data Engineer| 1040|\n",
      "|      Data Scientist|  840|\n",
      "|        Data Analyst|  612|\n",
      "|Machine Learning ...|  289|\n",
      "|  Analytics Engineer|  103|\n",
      "|      Data Architect|  101|\n",
      "|  Research Scientist|   82|\n",
      "|   Applied Scientist|   58|\n",
      "|Data Science Manager|   58|\n",
      "|   Research Engineer|   37|\n",
      "|         ML Engineer|   34|\n",
      "|        Data Manager|   29|\n",
      "|Machine Learning ...|   26|\n",
      "|Data Science Cons...|   24|\n",
      "|Data Analytics Ma...|   22|\n",
      "|Computer Vision E...|   18|\n",
      "|        AI Scientist|   16|\n",
      "|Business Data Ana...|   15|\n",
      "|     BI Data Analyst|   15|\n",
      "|     Data Specialist|   14|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------------------+\n",
      "|           job_title|       avg(salary)|\n",
      "+--------------------+------------------+\n",
      "|3D Computer Visio...|          120000.0|\n",
      "|  Lead Data Engineer|140333.33333333334|\n",
      "|        Data Modeler|          118900.0|\n",
      "| Data Scientist Lead|          134000.0|\n",
      "|Principal Data Ar...|         3000000.0|\n",
      "|Head of Machine L...|         6000000.0|\n",
      "|Machine Learning ...|          192420.0|\n",
      "|Data Analytics Sp...|           95000.0|\n",
      "|     Data Specialist|119642.85714285714|\n",
      "|Data Operations E...|           98485.0|\n",
      "|Deep Learning Res...|          115000.0|\n",
      "| Data Analytics Lead|          922500.0|\n",
      "|  Power BI Developer|          400000.0|\n",
      "|Machine Learning ...|163155.76923076922|\n",
      "|   Lead Data Analyst|          655000.0|\n",
      "|        BI Developer|129846.15384615384|\n",
      "|Staff Data Scientist|          105000.0|\n",
      "|       ETL Developer|          130947.0|\n",
      "|           Data Lead|          212500.0|\n",
      "|Product Data Scie...|            8000.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"job_title\")\\\n",
    "    .count()\\\n",
    "    .sort(\"count\", ascending=False)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           job_title|       avg(salary)|\n",
      "+--------------------+------------------+\n",
      "|Head of Machine L...|         6000000.0|\n",
      "|Principal Data Ar...|         3000000.0|\n",
      "|Lead Machine Lear...|2548666.6666666665|\n",
      "| Lead Data Scientist| 928485.3333333334|\n",
      "| Data Analytics Lead|          922500.0|\n",
      "|     BI Data Analyst|          836644.8|\n",
      "|Head of Data Science| 703729.4444444445|\n",
      "|   Lead Data Analyst|          655000.0|\n",
      "|         ML Engineer| 609997.9117647059|\n",
      "|Product Data Analyst|          412000.0|\n",
      "|  Power BI Developer|          400000.0|\n",
      "|Data Science Manager| 379390.4137931034|\n",
      "|Data Science Tech...|          375000.0|\n",
      "|   Big Data Engineer| 365909.0909090909|\n",
      "|Applied Machine L...| 306233.3333333333|\n",
      "|Applied Data Scie...|          283200.0|\n",
      "|        AI Scientist|          275312.5|\n",
      "|Machine Learning ...|          260750.0|\n",
      "|Business Data Ana...|          256200.0|\n",
      "|Cloud Data Architect|          250000.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.groupBy(\"job_title\")\\\n",
    "    .avg('salary')\\\n",
    "    .sort(\"avg(salary)\", ascending=False)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Aggregations and HAVING clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|           job_title|Num_Jobs|           AVG_Sal|\n",
      "+--------------------+--------+------------------+\n",
      "|     BI Data Analyst|      15|          836644.8|\n",
      "|         ML Engineer|      34| 609997.9117647059|\n",
      "|Data Science Manager|      58| 379390.4137931034|\n",
      "|   Big Data Engineer|      11| 365909.0909090909|\n",
      "|Applied Machine L...|      12| 306233.3333333333|\n",
      "|        AI Scientist|      16|          275312.5|\n",
      "|Business Data Ana...|      15|          256200.0|\n",
      "|      Data Scientist|     840|239073.47619047618|\n",
      "|Computer Vision E...|      18|224966.66666666666|\n",
      "|Director of Data ...|      11|198227.27272727274|\n",
      "|   Applied Scientist|      58| 190264.4827586207|\n",
      "|Machine Learning ...|     289|182216.03460207614|\n",
      "|Machine Learning ...|      26|163155.76923076922|\n",
      "|   Research Engineer|      37| 162752.8108108108|\n",
      "|      Data Architect|     101|161713.77227722772|\n",
      "|  Research Scientist|      82|160768.89024390245|\n",
      "|       Data Engineer|    1040|156574.96346153846|\n",
      "|  Analytics Engineer|     103| 151352.6213592233|\n",
      "|Data Science Cons...|      24|          141937.5|\n",
      "|Data Analytics Ma...|      22| 141879.0909090909|\n",
      "+--------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "\n",
    "df.groupBy(\"job_title\")\\\n",
    "    .agg(count(\"*\").alias(\"Num_Jobs\"), avg('salary').alias(\"AVG_Sal\"))\\\n",
    "    .where(col(\"Num_Jobs\") > 10)\\\n",
    "    .sort(\"AVG_Sal\", ascending=False)\\\n",
    "    .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Query in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+\n",
      "|           job_title|Num_Jobs|AVG_Sal|\n",
      "+--------------------+--------+-------+\n",
      "|     BI Data Analyst|      15| 836644|\n",
      "|         ML Engineer|      34| 609997|\n",
      "|Data Science Manager|      58| 379390|\n",
      "|   Big Data Engineer|      11| 365909|\n",
      "|Applied Machine L...|      12| 306233|\n",
      "|        AI Scientist|      16| 275312|\n",
      "|Business Data Ana...|      15| 256200|\n",
      "|      Data Scientist|     840| 239073|\n",
      "|Computer Vision E...|      18| 224966|\n",
      "|Director of Data ...|      11| 198227|\n",
      "|   Applied Scientist|      58| 190264|\n",
      "|Machine Learning ...|     289| 182216|\n",
      "|Machine Learning ...|      26| 163155|\n",
      "|   Research Engineer|      37| 162752|\n",
      "|      Data Architect|     101| 161713|\n",
      "|  Research Scientist|      82| 160768|\n",
      "|       Data Engineer|    1040| 156574|\n",
      "|  Analytics Engineer|     103| 151352|\n",
      "|Data Science Cons...|      24| 141937|\n",
      "|Data Analytics Ma...|      22| 141879|\n",
      "+--------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Temporary table in PySpark\n",
    "df.createOrReplaceTempView(\"Jobs\")\n",
    "\n",
    "sql_q = \\\n",
    "    \"SELECT job_title, \"\\\n",
    "        \"COUNT(*) AS Num_Jobs, \"\\\n",
    "        \"INT(AVG(salary)) AS AVG_Sal \"\\\n",
    "    \"FROM Jobs \"\\\n",
    "    \"GROUP BY job_title \"\\\n",
    "    \"HAVING Num_Jobs > 10 \"\\\n",
    "    \"ORDER BY AVG_Sal DESC\"\n",
    "\n",
    "spark.sql(sql_q).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp],\n",
       " DataFrame[a: bigint, b: bigint]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "\n",
    "my_df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "pd_df = pd.DataFrame({'a': [1,2,3], 'b': [1,2,3]})\n",
    "my_pd_df = spark.createDataFrame(pd_df)\n",
    "[my_df, my_pd_df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = ss.newSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ldm' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n ldm ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ee6cb9a9fcd54883ab7200357230507'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": {\n",
      "    \"name\": \"Ottawa\",\n",
      "    \"region\": \"Ontario\",\n",
      "    \"country\": \"Canada\",\n",
      "    \"lat\": 45.42,\n",
      "    \"lon\": -75.7,\n",
      "    \"tz_id\": \"America/Toronto\",\n",
      "    \"localtime_epoch\": 1694016710,\n",
      "    \"localtime\": \"2023-09-06 12:11\"\n",
      "  },\n",
      "  \"current\": {\n",
      "    \"last_updated_epoch\": 1694016000,\n",
      "    \"last_updated\": \"2023-09-06 12:00\",\n",
      "    \"temp_c\": 29.0,\n",
      "    \"temp_f\": 84.2,\n",
      "    \"is_day\": 1,\n",
      "    \"condition\": {\n",
      "      \"text\": \"Partly cloudy\",\n",
      "      \"icon\": \"//cdn.weatherapi.com/weather/64x64/day/116.png\",\n",
      "      \"code\": 1003\n",
      "    },\n",
      "    \"wind_mph\": 3.8,\n",
      "    \"wind_kph\": 6.1,\n",
      "    \"wind_degree\": 210,\n",
      "    \"wind_dir\": \"SSW\",\n",
      "    \"pressure_mb\": 1011.0,\n",
      "    \"pressure_in\": 29.86,\n",
      "    \"precip_mm\": 0.0,\n",
      "    \"precip_in\": 0.0,\n",
      "    \"humidity\": 66,\n",
      "    \"cloud\": 25,\n",
      "    \"feelslike_c\": 31.2,\n",
      "    \"feelslike_f\": 88.2,\n",
      "    \"vis_km\": 24.0,\n",
      "    \"vis_miles\": 14.0,\n",
      "    \"uv\": 8.0,\n",
      "    \"gust_mph\": 6.5,\n",
      "    \"gust_kph\": 10.4,\n",
      "    \"air_quality\": {\n",
      "      \"co\": 243.7,\n",
      "      \"no2\": 0.5,\n",
      "      \"o3\": 100.1,\n",
      "      \"so2\": 0.6,\n",
      "      \"pm2_5\": 9.5,\n",
      "      \"pm10\": 9.8,\n",
      "      \"us-epa-index\": 1,\n",
      "      \"gb-defra-index\": 1\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://api.weatherapi.com/v1/current.json\"\n",
    "\n",
    "querystring = {\"q\": \"Canada\", \"aqi\": \"yes\"}\n",
    "\n",
    "headers = {\n",
    "\t\"key\": API_KEY,\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "response.json()\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
